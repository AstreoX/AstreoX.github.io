<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Welcome to My Blog</title>
    <url>/2024/10/04/hello-blog/</url>
    <content><![CDATA[<h2 id="大家好，我是-Gskyer！"><a href="#大家好，我是-Gskyer！" class="headerlink" title="大家好，我是 Gskyer！"></a>大家好，我是 Gskyer！</h2><p>欢迎来到我的个人博客！</p>
<h3 id="博客内容"><a href="#博客内容" class="headerlink" title="博客内容"></a>博客内容</h3><p>在这个博客中，你将找到一些技术主题的分享，包括编程技巧、项目经验以及我的个人见解。我希望这些内容能对你有所帮助。</p>
<h3 id="新访客指南"><a href="#新访客指南" class="headerlink" title="新访客指南"></a>新访客指南</h3><ul>
<li><strong>浏览博客</strong>：你可以通过菜单栏浏览各类文章，或者使用搜索功能寻找特定主题。</li>
<li><strong>评论</strong>：如果你对我的文章有任何想法或问题，欢迎在下方留言。这个博客使用 Waline 评论系统，你可以轻松地参与讨论。</li>
</ul>
<h3 id="Waline-评论系统使用指南"><a href="#Waline-评论系统使用指南" class="headerlink" title="Waline 评论系统使用指南"></a>Waline 评论系统使用指南</h3><p>Waline 是一个轻量级的评论系统，使用起来非常简单。以下是如何使用 Waline 的步骤：</p>
<ol>
<li><p><strong>发表评论</strong>：</p>
<ul>
<li>在每篇文章的底部，你会看到评论框。输入你的昵称、电子邮件（可选）和评论内容。</li>
<li>点击“发送”按钮提交你的评论。</li>
</ul>
</li>
<li><p><strong>查看评论</strong>：</p>
<ul>
<li>提交后，你的评论将显示在评论区。你可以随时返回来查看和回复其他人的评论。</li>
</ul>
</li>
<li><p><strong>回复评论</strong>：</p>
<ul>
<li>如果你想回复某个评论，可以点击该评论下方的“回复”按钮，输入你的回复内容后提交。</li>
</ul>
</li>
<li><p><strong>编辑和删除评论</strong>：</p>
<ul>
<li>提交后的评论旁边通常会有“编辑”和“删除”选项。点击相应按钮即可进行操作（前提是你有相应权限）。</li>
</ul>
</li>
<li><p><strong>注意事项</strong>：</p>
<ul>
<li>请遵守社区规范，尊重他人，避免发布不当内容。</li>
<li>如果遇到任何问题，欢迎随时联系我或查看 Waline 的官方文档。</li>
</ul>
</li>
</ol>
<h3 id="特别感谢"><a href="#特别感谢" class="headerlink" title="特别感谢"></a>特别感谢</h3><p>我想特别感谢 Hexo 框架和 Redefine 主题，提供了如此优秀的工具，使我能够轻松创建和管理我的博客。</p>
<p>感谢你的光临，期待与你们的交流！</p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>Welcome</tag>
        <tag>self-introduction</tag>
      </tags>
  </entry>
  <entry>
    <title>大语言模型P1</title>
    <url>/2025/02/07/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BP1/</url>
    <content><![CDATA[<h1 id="北京大语言模型实践-Day-1"><a href="#北京大语言模型实践-Day-1" class="headerlink" title="北京大语言模型实践 Day 1"></a>北京大语言模型实践 Day 1</h1><p>​	 来到北京第一天，参加大语言模型的实践。</p>
<p>​	今天北京真的是格外的冷，零下19度，还有大风呼呼的吹，本来带了两件外套想着轮着穿，结果到最后一起穿在了身上，包的和太空人一样</p>
<hr>
<h2 id="课程资源介绍"><a href="#课程资源介绍" class="headerlink" title="课程资源介绍"></a>课程资源介绍</h2><p>我们的课程资源大概有这三个方面：</p>
<ol>
<li><p><strong>《Hands-On Large Language Models》</strong></p>
<p>这书倒是蛮新的，新到国内尚无中文译本（截至我们开课前2025.2.7）</p>
<p>而且手把手教着搓大语言模型，实践性很强，有配套的Github代码仓库</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/rasbt/LLMs-from-scratch</span><br></pre></td></tr></table></figure></div>


</li>
<li><p><strong>《大语言模型》</strong>（赵鑫&#x2F;李军毅&#x2F;周昆&#x2F;唐天一&#x2F;文继荣 著）</p>
</li>
</ol>
<p>   这书感觉就是更加理论化一点，而且因为是国内大佬编写的，所以也不像第一个英文原版一样读起来还有点困难，作为实践之中的理论补充还是非常不错的</p>
<ol start="3">
<li><strong>OpenBMB</strong></li>
</ol>
<p>   这是个携手清华打造的线上课程，分p很细，可以挑选自己感兴趣的着重学习一下，虽然这个主要是我们在实践前作为前置内容学习的，不过课程还是非常有质量有深度的，回来还是要再好好看一下</p>
<hr>
<p>ok，那么接下来就要进入课程了</p>
<p>一开课的惯例：课程介绍，老师介绍，大语言模型趋势，人工智能与机器学习的基本概念介绍</p>
<p>一大堆介绍完之后，终于来到了大语言模型的简介。这也算是我最感兴趣的内容，从了解到大语言模型之日起（大概是在22年11月份刚刚使用过GPT后不久），我就一直非常好奇这种LLM究竟是怎样理解我们人类的语言的，之前只是模模糊糊知道一点关于“token”的事情，知道那是个大概的大模型拆分语言的单元，今天总算能从头到尾去彻底理解一下了</p>
<p>（另外下面小小的感慨一下）</p>
<p>PPT正文首页就是“60年的算法积累，辛顿位列C位，能在60年前人类的CPU才刚刚买入集成电路的时代的算力条件下有勇气去搞神经网络，“先驱”一词在他身上大概就是最好的体现。 （另外，对于辛顿有一篇文章写的特别好，可以来看看<a class="link" href="https://www.zhihu.com/question/493793998/answer/1901401663572538161">为什么有些人能够那么强大又那么温柔？ - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>）</p>
<p><img lazyload src="/images/loading.svg" data-src="20250508014658701.png"></p>
<hr>
<p>好了言归正传，让我们首先来从大语言模型的历史开始：</p>
<p>在2017年划时代的Transformer架构发布之前，众多优秀的计算机科学家也在自然语言处理的道路上进行了深远的探索。</p>
<p><img lazyload src="/images/loading.svg" data-src="20250508014809451.png"></p>
<h3 id="1-词袋模型-（Bag-of-Words）"><a href="#1-词袋模型-（Bag-of-Words）" class="headerlink" title="1. 词袋模型 （Bag-of-Words）"></a>1. 词袋模型 （Bag-of-Words）</h3><h4 id="主要前提："><a href="#主要前提：" class="headerlink" title="主要前提："></a>主要前提：</h4><p>​		假设文本的语义主要由其中包含的词汇及其出现次数决定，而词的顺序和语法可以忽略。</p>
<h4 id="构建过程："><a href="#构建过程：" class="headerlink" title="构建过程："></a>构建过程：</h4><ul>
<li>首先需要从训练文本中提取所有不同的单词，形成一个词典</li>
</ul>
<p>​		再对于每一文本，根据词典中单词的出现次数，将文本转换为一个向量</p>
<ul>
<li><p>有点抽象不太好懂？</p>
<p>举个例子：</p>
<p>假设有两句话：“我爱自然语言处理”和“自然语言处理很有趣”。</p>
<p>构建词典时，会将所有不同的词列出：[“我”, “爱”, “自然语言处理”, “很”, “有趣”]。</p>
<p>第一句话对应的向量就是1,1,1,0,0，第二句话对应的向量是0,0,1,1,1。</p>
<p>词袋模型的处理就是这个样子，有点类似于关键词匹配</p>
<p><img lazyload src="/images/loading.svg" data-src="20250508014907527.png"></p>
<h4 id="评价："><a href="#评价：" class="headerlink" title="评价："></a>评价：</h4><p>这样子好处当然是有的，<strong>处理算法的简单带来了计算的高效</strong></p>
<p>但是缺点也显而易见：</p>
<ul>
<li><strong>丢失语序信息</strong>：词袋模型忽略了词的顺序，这可能导致对文本语义的误解，比如 “我喜欢她”和“她喜欢我”在词袋模型中表示相同，但这实际上就根本不是一回事</li>
<li><strong>维度灾难</strong>： 显而易见，当文本数据量大时，词典的维度可能会变得非常高，导致向量维度过大，增加计算复杂度。</li>
<li><strong>无法捕捉语义</strong>：无法捕捉词之间的语义关系和上下文信息，不同的词即使语义相似，在词袋模型中也会被表示为不同的维度。</li>
</ul>
</li>
</ul>
<h3 id="2-Word2Vec-（稠密向量-嵌入向量）"><a href="#2-Word2Vec-（稠密向量-嵌入向量）" class="headerlink" title="2.Word2Vec （稠密向量&#x2F;嵌入向量）"></a>2.Word2Vec （稠密向量&#x2F;嵌入向量）</h3><p>​	这我觉得是一项天才般的技术设计，巧妙地使程序具有了类似人类的语义理解功能，让我们一起来看一下：</p>
<ul>
<li><h4 id="主要假设："><a href="#主要假设：" class="headerlink" title="主要假设："></a>主要假设：</h4></li>
</ul>
<p>​		分布式语义假说（”You shall know a word by the company it keeps”）</p>
<p>​		也就是说，它信奉<strong>词的出现上下文决定词的语义</strong>假说，将每个词看成一个点，通过训练，让频繁一起出现的词在向量空间中距离更近，从而捕捉词的语义信息。 </p>
<ul>
<li><h4 id="我们举一个二维空间的例子来理解："><a href="#我们举一个二维空间的例子来理解：" class="headerlink" title="我们举一个二维空间的例子来理解："></a>我们举一个二维空间的例子来理解：</h4></li>
<li><p><img lazyload src="/images/loading.svg" data-src="20250508014943467.png"></p>
<p>如图所示我们可以看到有cats dog apple building adult等单词分布在这个空间中 其中 cats dog puppy都是小宠物一类的，在这个向量空间中就非常接近，聚成了一簇；同时我们观察另外几个类似的簇，都能找到类似的共同点 </p>
<p>那么，通过查找在向量空间中的距离远近，大语言模型就可以理解其中不同词的相关性，做出一种语义的理解。</p>
<p><img lazyload src="/images/loading.svg" data-src="20250508015024925.png"></p>
</li>
<li><p>想想看，我们人类的词典也是用一些词去解释另一个词，直到一些基本的词不能再拆分为显式的解释，而是一种模模糊糊的次级概念。对于大模型也是一样word2vec技术也并没有具体的把每个簇都显式地加上一个标签对吧，也是大模型的一种模模糊糊的<strong>次级概念理解</strong>。</p>
</li>
<li><p>那么在自然语言的处理过程中，将语言中的词去映射为低维，稠密的向量的过程，就叫做向量嵌入，也就是常说的embedding过程。</p>
</li>
<li><p>而且不仅是单词，实际上，<strong>不同层级的文本都可以做embedding</strong>，token层级，word层级，sentence层级，document层级上都可以做向量嵌入，单词的embedding就是单词的一个向量表示它的语义，句子的embedding就是这句话的语义向量（当然可能这句话的语义很复杂，但确实是可以的），文档的embedding就是这一大段的一个整体的语义。</p>
</li>
<li><p><img lazyload src="/images/loading.svg" data-src="20250508015051118.png"></p>
</li>
<li><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li><strong>一词多义问题：</strong> 像是charge这样有很多很不想干的释义就难以在向量空间中找到合适的对应位置</li>
<li><strong>缺少上下文信息</strong>：如像是it这种代词，单论语义很复杂，什么都有可能包含，需要去看上下文信息解读所代指的东西，但如果纯靠向量来解析语义就很困难</li>
</ul>
</li>
</ul>
<h3 id="3-RNN-Recurrent-Neural-Network）"><a href="#3-RNN-Recurrent-Neural-Network）" class="headerlink" title="3. RNN(Recurrent Neural Network）"></a>3. RNN(Recurrent Neural Network）</h3><p>​	那么为了解决上下文信息的问题，人们又引入了这玩意——循环神经网络</p>
<ul>
<li><p>RNN 具有<strong>记忆功能</strong>，它会对之前的信息进行记忆，将信息从当前步骤传递到下一步骤，形成一个<strong>循环结构</strong>，使得网络能够记住之前的信息，对于序列数据的处理非常有效。</p>
</li>
<li><p><img lazyload src="/images/loading.svg" data-src="20250508015117355.png"></p>
</li>
<li><p><img lazyload src="/images/loading.svg" data-src="20250508015129267.png"></p>
</li>
<li><p><img lazyload src="/images/loading.svg" data-src="20250508015208156.png"></p>
</li>
<li><p><img lazyload src="/images/loading.svg" data-src="20250508015218774.png"></p>
</li>
<li><p>(该图展示的为带注意力机制的RNN)</p>
</li>
<li><p>但是我们从他的运行方式就可以看到，它是一种<strong>循环顺序运行</strong>的方式，每次只处理一个附加的token，这种方式<strong>无法并行计算</strong>，就无法使其运行速度得到很大的提高。</p>
<p>而且这种序列化的处理方式也对训练带来了很多困难。如果训练的序列过长，就会有<strong>梯度消失</strong>或者<strong>梯度爆炸</strong>，难以捕捉长序列中的上下文关系。</p>
</li>
</ul>
<hr>
<h3 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h3><p>​	好了，<strong>Bag-of-Words</strong>带我们迈入了自然语言处理的大门，<strong>Word2Vec</strong>技术又通过巧妙的向量语义空间设置，使自然语言处理在语义理解方面甩开了大步子，<strong>RNN</strong>试图解决上下文的注意力问题，但方向似乎有点不对头，无法并行计算到最后终究只能靠大力出奇迹，但也走得很远啦，在一般的情感分析和机器翻译任务中也取得了不少傲人的成绩。</p>
<p>​	<strong>但似乎就差那么关键的一步了……</strong></p>
<p>​	一切都已就位，RNN的无法并行计算特点极大地限制了算力的释放，这成为了卡住大规模自然语言处理的最后一道枷锁。</p>
<p>​		舞台已经搭建好了，一切都静待着那划时代的架构登场了……</p>
]]></content>
      <categories>
        <category>AI</category>
        <category>大语言模型</category>
        <category>实践</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>大语言模型</tag>
        <tag>实践</tag>
      </tags>
  </entry>
</search>
